{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')\n",
    "shop = pd.read_csv('../input/shops.csv')\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "sales = pd.read_csv('../input/sales_train_v2.csv')\n",
    "items = pd.read_csv('../input/items.csv')\n",
    "item_category = pd.read_csv('../input/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data\n",
    "Since the test data is generated with combination of shops and items, we have to restructure train data to match the test data generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "#Compute all shops/items combinations\n",
    "grid = []\n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 60 unique shop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop.head(5)\n",
    "shop.describe()\n",
    "shop.shop_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 60 unique item_category_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_category.head(5)\n",
    "item_category.describe()\n",
    "item_category.item_category_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 22170 unique item_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head(5)\n",
    "items.describe()\n",
    "items.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the format for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main dataset for training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(5)\n",
    "sales.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of date_block_num against index. \n",
    "\n",
    "The distribution of each date_block_num is balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.date_block_num.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of shop_id against item_id on train and test data\n",
    "\n",
    "Train data consists of sales/returns only, while test data was constructed as a cartesian product of all shops and items in the given month (shops * items) - so it will include a lot of \"non-sales\" occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop_duplicates(subset=['item_id', 'shop_id']).plot.scatter('item_id', 'shop_id', color='DarkBlue', s = 0.1)\n",
    "test.drop_duplicates(subset=['item_id', 'shop_id']).plot.scatter('item_id', 'shop_id', color='DarkGreen', s = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after merging sales data on test data, there is about 50% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.merge(sales, how='left', on=['shop_id', 'item_id']).isnull().sum()\n",
    "\n",
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify outliers in sales data. We found that there are some high price sales such as Radmin 3: Reliable Remote Support Software which more than USD 3468 or 200000 Russian Ruble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.item_price.plot()\n",
    "sales[sales['item_price'] > 100000]\n",
    "items[items['item_id'] == 6066]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number of sales, there is an amount of sales more than 900. They are some kind of plastic bags and delivery services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.item_cnt_day.plot()\n",
    "sales[sales['item_cnt_day'] > 900]\n",
    "items[(items['item_id'] == 11373) & (items['item_id'] == 20949)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers above are removed so we only include item price that less than 100000 and amount of sales that less or equal to 900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[sales.item_price<100000]\n",
    "sales = sales[sales.item_cnt_day<=900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. The following code-cell serves just that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\n",
    "sales_m = pd.merge(grid,sales_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "\n",
    "sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')\n",
    "sales_m = pd.merge(sales_m,item_category, on=['item_category_id'], how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform mean encoding on all category data which are item_id, shop_id and item_category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_id in ['item_id','shop_id','item_category_id']:\n",
    "    for column_id, agg_func, func_name in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n",
    "\n",
    "        mean_df = sales_m.groupby([type_id,'date_block_num']).agg(agg_func).reset_index()[[column_id,type_id,'date_block_num']]\n",
    "        mean_df.columns = [type_id+'_'+func_name+'_'+column_id, type_id,'date_block_num']\n",
    "        \n",
    "        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num', type_id],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append test data into train data so we can create lag features on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = test.copy()\n",
    "temp_test['date_block_num'] = 34\n",
    "temp_test.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "temp_test = temp_test.merge(items, how='left', on='item_id')\n",
    "temp_test = temp_test.merge(item_category, how='left', on='item_category_id')\n",
    "temp_test.drop('item_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m.drop('item_name', axis=1, inplace=True)\n",
    "sales_m = pd.concat([sales_m,temp_test], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create lags on 10 features. 9 features are from mean encoding and 1 feature is the item_cnt_day.\n",
    "\n",
    "After several tries, we found that month lag intervals of 1, 2, 3, 4, 5, 6, 9 and 12 give best score in the leaderboard. Take note that 16GB of ram is unable to compute such large number of interval. We need at least 64GB of ram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_variables  = ['item_id_avg_item_price',\n",
    "'item_id_sum_item_cnt_day',\n",
    "'item_id_avg_item_cnt_day',\n",
    "'shop_id_avg_item_price',\n",
    "'shop_id_sum_item_cnt_day',\n",
    "'shop_id_avg_item_cnt_day',\n",
    "'item_category_id_avg_item_price',\n",
    "'item_category_id_sum_item_cnt_day',\n",
    "'item_category_id_avg_item_cnt_day',\n",
    "'item_cnt_day']\n",
    "\n",
    "#Limited by computation resource\n",
    "#lags = [1]\n",
    "lags = [1, 2, 3, 4, 5, 6, 9, 12]\n",
    "\n",
    "for lag in lags:\n",
    "    sales_new_df = sales_m.copy()\n",
    "    sales_new_df.date_block_num+=lag\n",
    "    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n",
    "    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n",
    "    sales_m = sales_m.merge(sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sales_m.columns:\n",
    "    if 'item_cnt' in feat:\n",
    "        sales_m[feat]=sales_m[feat].fillna(0)\n",
    "    elif 'item_price' in feat:\n",
    "        sales_m[feat]=sales_m[feat].fillna(sales_m[feat].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature we tried but not improving the score\n",
    "We tried generating text feature on item_category_name by TF-IDF. It slows down the training and still not giving good result in leaderboard. We run through grid search for optimizing the hyperparameters and it takes more than 2 days with i7-7700 and 64GB ram. Due to the limited time of accessing the higher spec computer, we give up this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_m['item_category_name'] = sales_m.item_category_name.str.replace(' - ', ' ')\n",
    "\n",
    "# count_vect = CountVectorizer(ngram_range=(1, 2), min_df=0.1)\n",
    "\n",
    "#Tfidf for item_category_name\n",
    "# X_train_counts = count_vect.fit_transform(sales_m.item_category_name)\n",
    "# tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "# X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "# header = ['cate_fea_' + str(i) for i in range(0, X_train_tf.toarray().shape[1])]\n",
    "# text_fea = pd.DataFrame(X_train_tf.toarray(),  columns=header)\n",
    "# sales_m = sales_m.join(text_fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop uneccesary columns and take data only after 12 since the most lag month interval is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = lag_variables[:-1] + ['item_price']\n",
    "sales_m = sales_m[sales_m['date_block_num']>12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "We use holdout scheme for cross validation. We use sales month from 12 to 32 for training, month 33 for validation and month 34 for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sales_m[sales_m['date_block_num']<33].drop(cols_to_drop, axis=1)\n",
    "X_cv =  sales_m[sales_m['date_block_num']==33].drop(cols_to_drop, axis=1)\n",
    "X_test = sales_m[sales_m['date_block_num']==34].drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference from discussion\n",
    "\n",
    "From discussion in Kaggle forum, we applied this useful trick, which clip the item_cnt_day within 40 instead of 20. In this way, there are more predictions on item_cnt_day = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['item_cnt_day'].clip_upper(40, inplace=True)\n",
    "X_train['item_cnt_day'].clip_lower(0, inplace=True)\n",
    "\n",
    "X_cv['item_cnt_day'].clip_upper(40, inplace=True)\n",
    "X_cv['item_cnt_day'].clip_lower(0, inplace=True)\n",
    "\n",
    "X_test['item_cnt_day'].clip_upper(40, inplace=True)\n",
    "X_test['item_cnt_day'].clip_lower(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_cv.to_csv('X_cv.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
